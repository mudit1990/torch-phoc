{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch.autograd\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import copy\n",
    "from datasets.maps_alt import MAPSDataset\n",
    "\n",
    "#from cnn_ws.transformations.homography_augmentation import HomographyAugmentation\n",
    "from cnn_ws.losses.cosine_loss import CosineLoss\n",
    "\n",
    "from cnn_ws.models.myphocnet import PHOCNet\n",
    "from cnn_ws.evaluation.retrieval import map_from_feature_matrix, map_from_query_test_feature_matrices\n",
    "from torch.utils.data.dataloader import _DataLoaderIter as DataLoaderIter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from cnn_ws.utils.save_load import my_torch_save, my_torch_load\n",
    "\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logger.warning('Could not find CUDA environment, using CPU mode')\n",
    "    gpu_id = None\n",
    "else:\n",
    "    gpu_id = [0]\n",
    "#torch.cuda.get_device_name(gpu_id[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = torch.load('PHOCNet_all_case.pt')\n",
    "cnn = model_.module#list(model_.named_parameters())\n",
    "if gpu_id is not None:\n",
    "        if len(gpu_id) > 1:\n",
    "            cnn = nn.DataParallel(cnn, device_ids=gpu_id)\n",
    "            cnn.cuda()\n",
    "        else:\n",
    "            cnn.cuda(gpu_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the file names\n",
    "f = open('../splits/val_files.txt', 'rb')\n",
    "A = f.readlines()\n",
    "f.close()\n",
    "A = [x.rstrip('\\n') for x in A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test images and words\n",
    "images_before = np.load('../../../detection_outputs_ready_for_test/detected_regions/'+A[0]+'.npy')\n",
    "words_before = np.load('../../../detection_outputs_ready_for_test/detected_labels/'+A[0]+'.npy')\n",
    "\n",
    "images_after = np.load('../../../detection_outputs_ready_for_test/new_regions/'+A[0]+'.npy')\n",
    "words_after = np.load('../../../detection_outputs_ready_for_test/new_labels/'+A[0]+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_before shape (536, 3, 135, 487)\n",
      "words_before shape (536,)\n",
      "images_after shape (469, 3, 135, 487)\n",
      "words_after shape (469,)\n"
     ]
    }
   ],
   "source": [
    "images_before = np.transpose(images_before, (0,3,1,2))\n",
    "images_after = np.transpose(images_after, (0,3,1,2))\n",
    "print 'images_before shape',images_before.shape\n",
    "print 'words_before shape',words_before.shape\n",
    "print 'images_after shape',images_after.shape\n",
    "print 'words_after shape',words_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [00:08<00:00, 62.74it/s]\n",
      "100%|██████████| 469/469 [00:07<00:00, 63.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert image tnto embedding using the cnn model\n",
    "def get_image_embeddings(images):\n",
    "    outputs = []\n",
    "    for i in tqdm(range(len(images))):\n",
    "        word_img = images[i]\n",
    "        word_img = 1 - word_img.astype(np.float32) / 255.0\n",
    "        word_img = word_img.reshape((1,) + word_img.shape)\n",
    "        word_img = torch.from_numpy(word_img).float()\n",
    "        word_img = word_img.cuda(gpu_id[0])\n",
    "        word_img = torch.autograd.Variable(word_img)\n",
    "        output = torch.sigmoid(cnn(word_img))\n",
    "        output = output.data.cpu().numpy().flatten()\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "outputs_before = get_image_embeddings(images_before)\n",
    "outputs_after = get_image_embeddings(images_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word variations\n",
    "# word_var is a dictionary that contains all variations as key and 0,1,-1 as value\n",
    "# 0 denotes the root word, -1 denotes var = root_word[:-1], +1 denotes var = root_word[1:]\n",
    "# root_word_var is a dict that stores original_word => all_variations\n",
    "def create_word_variations(words):\n",
    "    word_var = {}\n",
    "    root_word_var = {}\n",
    "    for w in words:\n",
    "        if len(w) < 2:\n",
    "            continue\n",
    "        root_var_list = [w.lower(), w.upper(), w.capitalize()]\n",
    "        var_set = set()\n",
    "        for var in root_var_list:\n",
    "            word_var[var] = 0\n",
    "            word_var[var[1:]] = 1\n",
    "            word_var[var[:-1]] = -1\n",
    "            var_set.add(var)\n",
    "            var_set.add(var[1:])\n",
    "            var_set.add(var[:-1])\n",
    "        root_word_var[w] = var_set\n",
    "    return word_var, root_word_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1919/1919 [00:00<00:00, 4572.74it/s]\n",
      " 17%|█▋        | 322/1871 [00:00<00:00, 3217.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('embedding variations:', (1919, 945))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1871/1871 [00:00<00:00, 4640.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('embedding variations:', (1871, 945))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the PHOC representation of the word itself\n",
    "from cnn_ws.string_embeddings.phoc import build_phoc_descriptor\n",
    "def get_word_phoc_representations(word_strings):\n",
    "    unigrams = [chr(i) for i in range(ord('&'), ord('&')+1) + range(ord('A'), ord('Z')+1) + \\\n",
    "                    range(ord('a'), ord('z') + 1) + range(ord('0'), ord('9') + 1)]\n",
    "    bigram_levels = None\n",
    "    bigrams = None\n",
    "    phoc_unigram_levels=(1, 2, 4, 8)\n",
    "    word_var_dir, root_word_var = create_word_variations(word_strings)\n",
    "    \n",
    "    word_var_strings = word_var_dir.keys()\n",
    "    embedding_var = build_phoc_descriptor(words=word_var_strings,\n",
    "                                  phoc_unigrams=unigrams,\n",
    "                                  bigram_levels=bigram_levels,\n",
    "                                  phoc_bigrams=bigrams,\n",
    "                                  unigram_levels=phoc_unigram_levels)\n",
    "    \n",
    "    print('embedding variations:', embedding_var.shape)\n",
    "    return (embedding_var, word_var_strings, word_var_dir, root_word_var)\n",
    "\n",
    "emb_info_before = get_word_phoc_representations(words_before)\n",
    "emb_info_after = get_word_phoc_representations(words_after)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new report matches method that handles variations\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "def report_matches_with_variations(outputs, matching, word_strings, emb_info, k, length):\n",
    "    # expand emb_info tuple\n",
    "    embedding_var, word_var_strings, word_var_dir, root_word_var = emb_info\n",
    "    # length sorting stuff\n",
    "    qualified_ids = [x for x in range(len(word_strings)) if len(word_strings[x]) > length]\n",
    "    outputs = np.array(outputs)\n",
    "    word_strings = np.array(word_strings)\n",
    "    outputs = list(outputs[qualified_ids])\n",
    "    word_strings = list(word_strings[qualified_ids])\n",
    "    \n",
    "    # same stuff for variations\n",
    "    qualified_ids_vars = [x for x in range(len(word_var_strings)) if len(word_var_strings[x]) > (length-1)]\n",
    "    embedding_var = np.array(embedding_var)\n",
    "    word_var_strings = np.array(word_var_strings)\n",
    "    embedding_var = list(embedding_var[qualified_ids_vars])\n",
    "    word_var_strings = list(word_var_strings[qualified_ids_vars])\n",
    "    \n",
    "    # the real computation\n",
    "    dist_mat = cdist(XA=outputs, XB=embedding_var, metric=matching)\n",
    "    retrieval_indices = np.argsort(dist_mat, axis=1)\n",
    "    q = retrieval_indices[:,:k]\n",
    "    count = 0\n",
    "    matched_words = []\n",
    "    img_dir = []\n",
    "    words_len = []\n",
    "    # get all matched words\n",
    "    for i in range(len(q)):\n",
    "        matched = []\n",
    "        for j in q[i]:\n",
    "            matched.append(word_var_strings[j])\n",
    "            curr_len = len(word_var_strings[j])\n",
    "            curr_dir = word_var_dir[word_var_strings[j]]\n",
    "            words_len.append(curr_len + abs(curr_dir))\n",
    "            img_dir.append(curr_dir)\n",
    "        matched_words.append(matched)\n",
    "    \n",
    "    # calculate accuracies\n",
    "    for i in range(len(word_strings)):\n",
    "        #print word_strings[i]\n",
    "        if word_strings[i] in matched_words[i]:\n",
    "            count = count+1\n",
    "        else:\n",
    "            for w in matched_words[i]:\n",
    "                if w in root_word_var[word_strings[i]]:\n",
    "                    count = count+1\n",
    "                    break\n",
    "\n",
    "    #q = np.squeeze(np.array(q))\n",
    "    #p = np.arange(len(q))\n",
    "    #print count\n",
    "    return (count, matched_words, qualified_ids, img_dir, words_len, outputs, word_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matches report\n",
    "match_report_before = report_matches_with_variations(outputs_before,'cosine',words_before,emb_info_before,1, 2)\n",
    "match_report_after = report_matches_with_variations(outputs_after,'cosine',words_after,emb_info_after,1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_qualified_ids, img_dir, matched_before, matched_after, ground_truth\n",
    "before_after_info = zip(match_report_before[2], match_report_before[3], match_report_before[1], match_report_after[1], match_report_before[6])\n",
    "before_after_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../../../before_after_extension_info/'+A[0]+'.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in before_after_info:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
