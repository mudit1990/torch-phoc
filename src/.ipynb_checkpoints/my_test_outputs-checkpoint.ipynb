{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch.autograd\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "import copy\n",
    "#from datasets.maps_alt import MAPSDataset\n",
    "#from maps_alt import MAPSDataset\n",
    "from datasets.maps_alt import MAPSDataset\n",
    "\n",
    "#from cnn_ws.transformations.homography_augmentation import HomographyAugmentation\n",
    "from cnn_ws.losses.cosine_loss import CosineLoss\n",
    "\n",
    "from cnn_ws.models.myphocnet import PHOCNet\n",
    "from cnn_ws.evaluation.retrieval import map_from_feature_matrix, map_from_query_test_feature_matrices\n",
    "from torch.utils.data.dataloader import _DataLoaderIter as DataLoaderIter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from cnn_ws.utils.save_load import my_torch_save, my_torch_load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cnn_ws.string_embeddings.phoc import build_phoc_descriptor\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "# from dist_func import damerau_levenshtein_distance as dld\n",
    "\n",
    "def report_matches(outputs, embedding, matching, word_strings, original_words, k, length, is_lower):\n",
    "    # length sorting stuff\n",
    "    qualified_ids = [x for x in range(len(word_strings)) if len(word_strings[x]) > length]\n",
    "    qualified_ids_original = [x for x in range(len(original_words)) if len(original_words[x]) > length]\n",
    "    outputs = np.array(outputs)\n",
    "    embedding = np.array(embedding)\n",
    "    word_strings = np.array(word_strings)\n",
    "    original_words = np.array(original_words)\n",
    "    outputs = list(outputs[qualified_ids_original])\n",
    "    embedding = list(embedding[qualified_ids])\n",
    "    word_strings = list(word_strings[qualified_ids])\n",
    "    original_words = list(original_words[qualified_ids_original])\n",
    "    # the real computation\n",
    "    dist_mat = cdist(XA=outputs, XB=embedding, metric=matching)\n",
    "    retrieval_indices = np.argsort(dist_mat, axis=1)\n",
    "    q = retrieval_indices[:,:k]\n",
    "    count = 0\n",
    "    matched_words = []\n",
    "    # get all matched words\n",
    "    #print len(outputs), len(embedding)\n",
    "    for i in range(len(q)):\n",
    "        matched = []\n",
    "        #print q[i]\n",
    "        for j in q[i]:\n",
    "            matched.append(word_strings[j])\n",
    "        matched_words.append(matched)\n",
    "    \n",
    "    #print len(word_strings), len(matched_words)\n",
    "    close_counts = 0\n",
    "    flag_match = np.zeros(len(original_words))\n",
    "    for i in range(len(original_words)):\n",
    "        #print original_words[i], matched_words[i]\n",
    "        if is_lower:\n",
    "            #print original_words[i], matched_words[i]\n",
    "            if original_words[i] in matched_words[i]:\n",
    "                #print \"yes\"\n",
    "                count = count+1\n",
    "                flag_match[i] = 1\n",
    "                #print \"matched: \"+str(bool(flag_match[i]))\n",
    "           # else:\n",
    "                #print \"no\"\n",
    "#                 pass\n",
    "#                 for j in matched_words[i]:\n",
    "#                     #print original_words[i], j, dld(original_words[i], j)\n",
    "#                     if dld(original_words[i], j) == 1:\n",
    "#                         close_counts = close_counts+1\n",
    "#                 pass\n",
    "        else:\n",
    "            if original_words[i].lower() in [str(x.lower()) for x in matched_words[i]]:\n",
    "                count = count+1\n",
    "                flag_match[i] = 1\n",
    "                \n",
    "    #print count\n",
    "    return count, close_counts, matched_words, outputs, embedding, word_strings, qualified_ids_original, flag_match\n",
    "\n",
    "def learning_rate_step_parser(lrs_string):\n",
    "    return [(int(elem.split(':')[0]), float(elem.split(':')[1])) for elem in lrs_string.split(',')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnn_ws/models/myphocnet.py:88: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias.data, 0)\n",
      "100%|██████████| 2069/2069 [00:00<00:00, 6867.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2069, 540)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters ######################################################################################\n",
    "embedding_type = 'phoc'\n",
    "bigram_levels = None\n",
    "bigrams = None\n",
    "phoc_unigram_levels = (1,2,4,8)\n",
    "fixed_image_size = None\n",
    "min_image_width_height = 26\n",
    "is_lower = 1\n",
    "\n",
    "# get the model ####################################################################################\n",
    "if is_lower:\n",
    "    n_out = 540\n",
    "else:\n",
    "    n_out = 945\n",
    "cnn = PHOCNet(n_out=n_out,\n",
    "    input_channels=3,\n",
    "    gpp_type='gpp',\n",
    "    pooling_levels=([1], [5]))\n",
    "\n",
    "cnn.init_weights()\n",
    "\n",
    "if is_lower == 1:\n",
    "    model_ = torch.load('PHOCNet_final.pt')\n",
    "else:\n",
    "    model_ = torch.load('PHOCNet_all_case.pt')\n",
    "\n",
    "cnn = model_.module\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.warning('Could not find CUDA environment, using CPU mode')\n",
    "    gpu_id = None\n",
    "else:\n",
    "    gpu_id = [0]\n",
    "\n",
    "if gpu_id is not None:\n",
    "        if len(gpu_id) > 1:\n",
    "            cnn = nn.DataParallel(cnn, device_ids=gpu_id)\n",
    "            cnn.cuda()\n",
    "        else:\n",
    "            cnn.cuda(gpu_id[0])\n",
    "\n",
    "# find the file names\n",
    "f = open('../splits/val_files.txt', 'rb')\n",
    "A = f.readlines()\n",
    "f.close()\n",
    "A = [x.rstrip('\\n') for x in A]\n",
    "\n",
    "# load test images and words\n",
    "images = np.load('../../../ProcessedData/original_images_nopad_'+A[1]+'.tiff.npy')\n",
    "words = np.load('../../../ProcessedData/original_words_nopad_'+A[1]+'.tiff.npy')\n",
    "\n",
    "# filter false poitives\n",
    "useful_ids = []\n",
    "for i in range(len(words)):\n",
    "    if words[i]  == 'no label attached':\n",
    "        pass\n",
    "    else:\n",
    "        useful_ids.append(i)\n",
    "\n",
    "images = images[useful_ids]\n",
    "words = words[useful_ids]\n",
    "\n",
    "original_words = np.copy(words)\n",
    "\n",
    "if is_lower == 1:\n",
    "    new_words = []\n",
    "    for i in words:\n",
    "        new_words.append(i.lower())\n",
    "    words = np.array(new_words)\n",
    "    original_words = np.copy(words)\n",
    "else:    \n",
    "    words = list(words)\n",
    "    new_words = []\n",
    "    for i in words:\n",
    "        new_words.append(i.upper())\n",
    "        new_words.append(i.lower())\n",
    "        new_words.append(i.capitalize())\n",
    "    words = np.array(new_words)\n",
    "\n",
    "# convert dimensions\n",
    "images = np.transpose(images, (0,3,1,2))\n",
    "images.shape\n",
    "\n",
    "# check if this works\n",
    "outputs = []\n",
    "for i in range(len(images)):\n",
    "    word_img = images[i]\n",
    "    word_img = 1 - word_img.astype(np.float32) / 255.0\n",
    "    word_img = word_img.reshape((1,) + word_img.shape)\n",
    "    word_img = torch.from_numpy(word_img).float()\n",
    "    word_img = word_img.cuda(gpu_id[0])\n",
    "    word_img = torch.autograd.Variable(word_img)\n",
    "    output = torch.sigmoid(cnn(word_img))\n",
    "    output = output.data.cpu().numpy().flatten()\n",
    "    outputs.append(output)\n",
    "\n",
    "# compute the PHOC representation of the word itself\n",
    "word_strings = words\n",
    "if is_lower == 0:\n",
    "    unigrams = [chr(i) for i in range(ord('&'), ord('&')+1) + range(ord('A'), ord('Z')+1) + \\\n",
    "                    range(ord('a'), ord('z') + 1) + range(ord('0'), ord('9') + 1)]\n",
    "else:\n",
    "    unigrams = [chr(i) for i in range(ord('a'), ord('z') + 1) + range(ord('0'), ord('9') + 1)]\n",
    "\n",
    "\n",
    "if is_lower == 1:\n",
    "    for i in range(len(word_strings)):\n",
    "        word_strings[i] = str(word_strings[i].lower())\n",
    "else:\n",
    "    pass\n",
    "\n",
    "embedding = build_phoc_descriptor(words=word_strings,\n",
    "                                  phoc_unigrams=unigrams,\n",
    "                                  bigram_levels=bigram_levels,\n",
    "                                  phoc_bigrams=bigrams,\n",
    "                                  unigram_levels=phoc_unigram_levels)\n",
    "\n",
    "print embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output visualization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'dld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eea5a375a76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_word_strings\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mqualified_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_strings\u001b[0m\u001b[0;34m,\u001b[0m                                        \u001b[0moriginal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_lower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"the accuracy is: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"the close_count accuracy is: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d6d72d7cdbcf>\u001b[0m in \u001b[0;36mreport_matches\u001b[0;34m(outputs, embedding, matching, word_strings, original_words, k, length, is_lower)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatched_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;31m#print original_words[i], j, dld(original_words[i], j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mdld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                         \u001b[0mclose_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclose_counts\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'dld' is not defined"
     ]
    }
   ],
   "source": [
    "count, close_count, matched_words, new_outputs, new_embedding, new_word_strings, \\\n",
    "        qualified_ids, flag_match = report_matches(outputs, embedding, 'cosine', word_strings, \\\n",
    "                                       original_words, k=2, length=3, is_lower=is_lower)\n",
    "\n",
    "print \"the accuracy is: \"+str(count/float(len(qualified_ids)))\n",
    "print \"the close_count accuracy is: \"+str(close_count/float(len(qualified_ids)))\n",
    "\n",
    "_len = min(500, len(matched_words))\n",
    "new_images = images[qualified_ids]\n",
    "new_strings = word_strings[qualified_ids]\n",
    "for i in range(_len):\n",
    "    print \"************************************************************************\"\n",
    "    print \"************************************************************************\"\n",
    "    print \"Original image:\"\n",
    "    q = np.transpose(new_images[i],(1,2,0))\n",
    "    #plt.imshow(q)\n",
    "    #plt.show()\n",
    "    print \"IoU matched word is: \"+str(new_strings[i])\n",
    "    print \"the matched words are (inorder): \"+str(matched_words[i])\n",
    "    print \"did it match?: \"+str(bool(flag_match[i]))\n",
    "    print \"------------------------------------------------------------------------\"\n",
    "    print \"------------------------------------------------------------------------\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
